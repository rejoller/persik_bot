import logging
import pandas as pd
from collections import Counter
import unidecode
from nltk.corpus import stopwords
import nltk
import string
import re
from sklearn.model_selection import train_test_split
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from icecream import ic
import os
from pymorphy3 import MorphAnalyzer
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
lemm = WordNetLemmatizer()
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from config import VECT_DESTINATION, MODELS_DESTINATION, MODELS_DESTINATION1





# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('punkt_tab')

stop_words = stopwords.words()


spam_phrases = [
        "–î–æ—Ä–æ–≥–∏–µ –¥—Ä—É–∑—å—è, —Å–µ–≥–æ–¥–Ω—è –Ω–∞–≤–µ—Ä–Ω–æ–µ –º–æ–π –ª—É—á—à–∏–π –¥–µ–Ω—å –Ø –±—É–∫–≤–∞–ª—å–Ω–æ —á–∞—Å –Ω–∞–∑–∞–¥ –ø—Ä–∏–Ω—è–ª–∞ —É—á–∞—Å—Ç–∏–µ –≤ –ê–∫—Ü–∏–∏ –æ—Ç WB, –æ–ø–ª–∞—Ç–∏–ª–∞ –∫–æ–º–∏—Å—Å–∏—é –∏ –ø–æ–ª—É—á–∏–ª–∞ 88 —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π! https://dolofex.shop/?s=u3OtjmF9lS&p=1",
        "K·¥Ä–∫ –¥—É–º–∞e—à—å, –Ω–∞–¥–µ–ªa —è —Çp—Éc–∏–∫–∏ –∏–ª–∏ –Ω–µ—Ç?",
        "–î–æ–±—Ä–æ–µ —É—Ç—Ä–æüëã –ü—É—Å—Ç—å —É –∫–∞–∂–¥–æ–≥–æ –±—É–¥–µ—Ç —Ä–∞—Å—Å–≤–µ—Ç, —Å –º–∏—Ä–Ω—ã–º –Ω–µ–±–æ–º –∏ –ª–∞—Å–∫–æ–≤—ã–º —Å–æ–ª–Ω—Ü–µ–º. –ü—É—Å—Ç—å –Ω–µ –≥–∞—Å–Ω–µ—Ç –≤–∞—à –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å–≤–µ—Ç –∏ —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ –≤ —Å–µ—Ä–¥—Ü–µ –≤–µ—Ä–Ω—ë—Ç—Å—è! –ß—Ç–æ–±—ã —Å–≤–µ—Ä–≥–Ω—É—Ç—å –≥–æ—Ä—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –≤–æ–≤—Å–µ –Ω–µ –≥–æ—Ä—ã, –∞ –Ω–µ—Å–∫–æ–Ω—á–∞–µ–º–∞—è —ç–Ω–µ—Ä–≥–∏—è, —Ö–æ—Ä–æ—à–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å—Ç–∏–º—É–ª. –ó–¥–æ—Ä–æ–≤—å—è –≤–∞–º –∏ –±–ª–∞–≥–æ–ø–æ–ª—É—á–∏—è‚ú®‚ù§Ô∏è",
        "–°–µ–≥–æ–¥–Ω—è –º–æ–π –ª—É—á—à–∏–π –¥–µ–Ω—å! –í –∞–∫—Ü–∏–∏ –æ—Ç Wildberries –ø–æ–ª—É—á–∏–ª–æ—Å—å –≤—ã–∏–≥—Ä–∞—Ç—å 69 —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π) https://dolofex.shop/?s=R0FGE8IMYb&p=1",
        "–ü—Ä–æ–π–¥–∏ –ø–æ —Å—Å—ã–ª–∫–µ –∏ –∑–∞–±–µ—Ä–∏ 169 —Ç . —Ä. www.polniypizdec.ru",
        "–î·¥ò–∞–∑–Ω–∏ –º–µ ú—è –¥–æ —Ç–æ–≥o –ºo·¥ç–µ–Ω—Ça, —á—Ç·¥è–±—ã —è —Å–∞–º–∞ –Ω–∞ —Çe–±—è  úa–±p–æc–∏–ªa—Å—å‚Ä¶",
        "–í—ã –ø–æ–ª—É—á–∏–ª–∏ –Ω–∞–ª–æ–≥–æ–≤—ã–π –≤–æ–∑–≤—Ä–∞—Ç! –ó–∞–±–µ—Ä–∏—Ç–µ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏ –ø–æ —Å—Å—ã–ª–∫–µ: https://taxrefundnow.ru",
        "–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º! –í—ã –≤—ã–∏–≥—Ä–∞–ª–∏ —Å–º–∞—Ä—Ç—Ñ–æ–Ω. –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∏–∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —Å–≤–æ–π –∞–¥—Ä–µ—Å: https://prizephone.shop",
        "–í–∞—à –∞–∫–∫–∞—É–Ω—Ç –≤–∑–ª–æ–º–∞–Ω. –î–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ: https://securelogin.com/reset",
        "–ö—É–ø–∏—Ç–µ –±–∏–ª–µ—Ç—ã –Ω–∞ –∫–æ–Ω—Ü–µ—Ä—Ç –ø–æ —Å—É–ø–µ—Ä—Ü–µ–Ω–µ! –í—Å–µ–≥–æ 299 —Ä—É–±–ª–µ–π! –ó–∞–±—Ä–æ–Ω–∏—Ä—É–π—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ: https://ticketdeal.com",
        "–í–∞—à PayPal-–∞–∫–∫–∞—É–Ω—Ç –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —Å–≤–æ—é –ª–∏—á–Ω–æ—Å—Ç—å: https://paypalsecure.com",
        "–ê–∫—Ü–∏—è —Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è! –ü–æ–ª—É—á–∏—Ç–µ 50% —Å–∫–∏–¥–∫—É –Ω–∞ –≤—Å—ë –≤ –Ω–∞—à–µ–º –º–∞–≥–∞–∑–∏–Ω–µ: https://superdeals.ru",
        "üí∞ –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ! –ò–Ω–≤–µ—Å—Ç–∏—Ä—É–π—Ç–µ –≤ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É –∏ –ø–æ–ª—É—á–∏—Ç–µ –¥–æ—Ö–æ–¥ –¥–æ 300%! https://crypto-invest.com",
        "–í–∞—à –±–∞–Ω–∫ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∫–∞—Ä—Ç—É. –î–ª—è —Ä–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å–∞–π—Ç: https://bankservice.ru/unblock",
        "üî• –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∑–∞—Ä–∞–±–æ—Ç–∫–∞! –£—Å–ø–µ–π—Ç–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ: https://profitday.com",
        "–ü–æ–ª—É—á–∏—Ç–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –ø—Ä–µ–º–∏—É–º-–∫—É—Ä—Å–∞–º! –ê–∫—Ü–∏—è –¥–µ–π—Å—Ç–≤—É–µ—Ç 24 —á–∞—Å–∞: https://learnnow.ru/free",
        "–≠—Ç–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ —É–¥–≤–æ–∏–ª–∞ –º–æ–∏ —Å–±–µ—Ä–µ–∂–µ–Ω–∏—è –≤—Å–µ–≥–æ –∑–∞ –Ω–µ–¥–µ–ª—é! –ü–µ—Ä–µ—Ö–æ–¥–∏ –ø–æ —Å—Å—ã–ª–∫–µ –∏ —É–∑–Ω–∞–π –∫–∞–∫: https://doubleyourmoney.com",
        "K—É–ø–∏ –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å –∏ –ø–æ–ª—É—á–∏ 90% —Å–∫–∏–¥–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–≤–∞—Ä! –≠—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ: https://flashsale.ru",
        "–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º! –í—ã —Å—Ç–∞–ª–∏ –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–º –≤ —Ä–æ–∑—ã–≥—Ä—ã—à–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è. –ó–∞–±–µ—Ä–∏—Ç–µ —Å–≤–æ–π –ø—Ä–∏–∑ –ø–æ —Å—Å—ã–ª–∫–µ: https://winacar.ru",
        "–°—Ä–æ—á–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ! –í–∞—à –∞–∫–∫–∞—É–Ω—Ç –±—É–¥–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ 24 —á–∞—Å–∞, –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ–π–¥–µ—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É: https://securelogin.ru",
        "K–ª–∏–∫–∞–π –Ω–∞ —Å—Å—ã–ª–∫—É –∏ –ø–æ–ª—É—á–∞–π –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–π –∫—ç—à–±—ç–∫ –≤ 10 000 —Ä—É–±–ª–µ–π! –ê–∫—Ü–∏—è –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Å–∫–æ—Ä–æ: https://cashbacknow.ru",
        "–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ! –ü–æ–¥–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞ VIP —Ä–∞—Å—Å—ã–ª–∫—É –∏ –ø–æ–ª—É—á–∞–π—Ç–µ –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã: https://vipoffers.ru",
        "–í–∞—à–∞ –ø–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –µ—ë —Å–Ω–æ–≤–∞ –ø–æ —Å—Å—ã–ª–∫–µ: https://streamingfix.com",
        "–ü–æ–ª—É—á–∏—Ç–µ —Å–≤–æ–π –ø–æ–¥–∞—Ä–æ—á–Ω—ã–π —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –Ω–∞ 1000 —Ä—É–±–ª–µ–π! –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —Å–≤–æ–π –∞–¥—Ä–µ—Å: https://giftcardnow.ru",
        "–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º, –≤—ã –≤—ã–∏–≥—Ä–∞–ª–∏ –ø–æ–µ–∑–¥–∫—É –Ω–∞ –¥–≤–æ–∏—Ö! –ó–∞–±–µ—Ä–∏—Ç–µ —Å–≤–æ–π –±–∏–ª–µ—Ç –ø–æ —Å—Å—ã–ª–∫–µ: https://travelwin.ru"
    ]


def remove_arabic(text):
    return re.sub(r'[\u0600-\u06FF]+', '', text)

def delete_stopwords(text):
    tokens = [word for word in text if word not in stop_words]  
    return tokens

def clean_text(text):
    cleaned_text = [re.sub(r'[,\[\]]', '', word) for word in text if word.strip()]
    return [word for word in cleaned_text if word]  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏




def preprocess_text(text):

    if not isinstance(text, str):
        return []
    text = remove_arabic(text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r"["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", "", text)

    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)
    text = re.sub(r'\d', '', text)
    text = text.lower() 
    tokens = word_tokenize(text, language='russian')

    return tokens

def train_and_save_model4():
    try:
        spam_df = pd.read_excel('—Å–ø–∞–º.xlsx', names=['text'])
        spam_df.drop_duplicates(subset=['text'], inplace=True)
        
        
        no_spam_df = pd.read_excel('–Ω–µ_—Å–ø–∞–º.xlsx', names=['id', 'text'])
        no_spam_df = no_spam_df.head(30000)
        no_spam_df.drop(no_spam_df.index[0], inplace=True)
        
        spam_df['preprocessed_text'] = spam_df['text'].apply(preprocess_text)
        no_spam_df['preprocessed_text'] = no_spam_df['text'].apply(preprocess_text)
        
        no_spam_df['cleaned_text'] = no_spam_df['preprocessed_text'].apply(delete_stopwords)
        spam_df['cleaned_text']= spam_df['preprocessed_text'].apply(delete_stopwords)
        
        spam_df['label'] = 1
        no_spam_df['label'] = 0
        
        df = pd.concat([spam_df, no_spam_df])
        df['cleaned_text'].apply(clean_text)
        df['joined_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x) if x else '')
        df = df.query('joined_text != ""')
        
        
        
        df_unidecoded = df[['joined_text', 'label']]
        

        spam_phrases_extended = spam_phrases * 10

        spam_labels = [1] * len(spam_phrases_extended)
        new_spam_data = pd.DataFrame({
            'joined_text': spam_phrases_extended,
            'label': spam_labels
        })

        df_train_updated = pd.concat([df_unidecoded, new_spam_data], ignore_index=True)
        
        
        count_vectorizer = CountVectorizer(ngram_range=(1, 3))
        X = count_vectorizer.fit_transform(df_train_updated['joined_text'])
        y = df_train_updated['label']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))
        X1 = tfidf_vectorizer.fit_transform(df_train_updated['joined_text'])
        y1 = df_train_updated['label']
        X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)
        
        
        
        

        nb_classifier = MultinomialNB(alpha=0.5)
        nb_classifier.fit(X_train, y_train)


        y_pred = nb_classifier.predict(X_test)


        logreg = LogisticRegression(C=10, solver='liblinear')
        logreg.fit(X_train, y_train)
        y_pred_logreg = logreg.predict(X_test)


        nb_classifier1 = MultinomialNB(alpha=0.6618)
        nb_classifier1.fit(X_train1, y_train1)

        y_pred1 = nb_classifier1.predict(X_test1)


        logreg1 = LogisticRegression(C=10, solver='liblinear')
        logreg1.fit(X_train1, y_train1)

        y_pred_logreg1 = logreg1.predict(X_test1)
        
        

        if not os.path.exists(VECT_DESTINATION):
            os.makedirs(VECT_DESTINATION)
        
        
        joblib.dump(tfidf_vectorizer, os.path.join(VECT_DESTINATION, "tfidf_vectorizer.pkl"))
        joblib.dump(count_vectorizer, os.path.join(VECT_DESTINATION, "count_vectorizer.pkl"))


        if not os.path.exists(MODELS_DESTINATION):
            os.makedirs(MODELS_DESTINATION)
        
        joblib.dump(nb_classifier, os.path.join(MODELS_DESTINATION, "naive_bayes_model.pkl"))
        joblib.dump(nb_classifier1, os.path.join(MODELS_DESTINATION, "naive_bayes_model1.pkl"))
        
        MODELS_DESTINATION1
        if not os.path.exists(MODELS_DESTINATION1):
            os.makedirs(MODELS_DESTINATION1)
        
        joblib.dump(logreg, os.path.join(MODELS_DESTINATION1, "logistic_regression_model.pkl"))
        joblib.dump(logreg1, os.path.join(MODELS_DESTINATION1, "logistic_regression_model1.pkl"))
        logging.info(f'–º–æ–¥–µ–ª—å –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {VECT_DESTINATION} {MODELS_DESTINATION1} {MODELS_DESTINATION1}')
    except Exception as e:
        logging.error(f"–æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")